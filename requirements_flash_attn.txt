# Requirements with flash_attn enabled for PyTorch 2.6 + CUDA 12.6
# This replaces the flash-attention-disabled setup with proper flash_attn support

# Core ML libraries with flash attention support
torch==2.6.0
torchvision==0.21.0
torchaudio==2.6.0
transformers==4.52.3  # Updated to latest compatible version

# Flash attention - the main package we need to restore
flash-attn==2.8.0.post2

# Alternative attention mechanisms (keep as fallback)
xformers==0.0.28.post3

# Gradio and web interface
gradio==4.44.1
gradio_client==0.16.4

# Audio processing
librosa==0.9.2
soundfile==0.12.1
audioread==3.0.1
pydub==0.25.1

# Video processing
opencv-python==4.11.0.86
decord==0.6.0
imageio==2.37.0
imageio-ffmpeg==0.4.9
moviepy==1.0.3

# ML utilities
numpy==1.23.5
scipy==1.15.3
scikit-learn==1.6.1
einops==0.7.0
accelerate==1.6.0

# Hugging Face ecosystem
huggingface-hub==0.30.2
tokenizers==0.21.1
datasets==3.5.0

# Computer vision
timm==1.0.15
open-clip-torch==2.32.0

# Other dependencies
pillow==10.2.0
pandas==2.0.2
matplotlib==3.10.3
tqdm==4.67.1
pyyaml==6.0.2
requests==2.32.3
aiohttp==3.11.18
fastapi==0.115.12
uvicorn==0.34.2
python-dotenv==1.0.1