# Compatible flash-attention for PyTorch 2.6 + CUDA 12.6
# Use these exact wheel files to resolve ABI issues

# Option 1: Exact match for current environment
https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.0.post2/flash_attn-2.8.0.post2+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

# Option 2: Newer version (recommended)
https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.0.post2/flash_attn-2.8.0.post2+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl