# CLAUDE.md
<!-- Generated by Claude Conductor v1.1.2 -->

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Critical Context (Read First)
- **Tech Stack**: PyTorch, Gradio, Transformers, Diffusion Models (Flow Matching)
- **Main File**: app.py (174 lines) - Gradio web interface
- **Core Mechanic**: Any2Audio generation with Chain-of-Thought reasoning guided by MLLMs
- **Key Integration**: HuggingFace, ModelScope, PyTorch Lightning, CLIP, T5
- **Platform Support**: GPU-accelerated, Docker containerized, web-based demo
- **DO NOT**: Modify model architectures without understanding flow matching, skip memory optimization flags

## Session Startup Checklist
**IMPORTANT**: At the start of each session, check these items:
1. **Check TASKS.md** - Look for any IN_PROGRESS or BLOCKED tasks from previous sessions
2. **Review recent JOURNAL.md entries** - Scan last 2-3 entries for context
3. **If resuming work**: Load the current task context from TASKS.md before proceeding

## Table of Contents
1. [Architecture](ARCHITECTURE.md) - Tech stack, folder structure, infrastructure
2. [Design Tokens](DESIGN.md) - Colors, typography, visual system
3. [UI/UX Patterns](UIUX.md) - Components, interactions, accessibility
4. [Runtime Config](CONFIG.md) - Environment variables, feature flags
5. [Data Model](DATA_MODEL.md) - Database schema, entities, relationships
6. [API Contracts](API.md) - Endpoints, request/response formats, auth
7. [Build & Release](BUILD.md) - Build process, deployment, CI/CD
8. [Testing Guide](TEST.md) - Test strategies, E2E scenarios, coverage
9. [Operational Playbooks](PLAYBOOKS/DEPLOY.md) - Deployment, rollback, monitoring
10. [Contributing](CONTRIBUTING.md) - Code style, PR process, conventions
11. [Error Ledger](ERRORS.md) - Critical P0/P1 error tracking
12. [Task Management](TASKS.md) - Active tasks, phase tracking, context preservation

## Quick Reference
**Main Entry Points**:
- **app.py:1-174** - Gradio web interface and demo server
- **predict.py:1-163** - Core inference engine and audio generation
- **extract_latents.py:1-215** - Feature extraction pipeline (CLIP, T5, SynchFormer)

**Core Models**:
- **ThinkSound/models/diffusion.py:1-500** - MM-DiT diffusion transformer
- **ThinkSound/models/autoencoders.py:1-300** - Oobleck VAE encoder/decoder
- **ThinkSound/models/conditioners.py:400-600** - T5 and CLIP conditioning
- **ThinkSound/models/factory.py:1-200** - Model loading and configuration

**Data Processing**:
- **data_utils/v2a_utils/feature_utils_224.py:15-180** - Video/audio feature extraction
- **data_utils/ext/synchformer/synchformer.py:1-400** - Audio-visual synchronization
- **ThinkSound/inference/generation.py:1-300** - Sampling and generation logic

**Configuration & Scripts**:
- **requirements.txt:1-250** - Dependencies (PyTorch 2.6.0, Transformers 4.52.3)
- **setup.py:1-44** - Package installation (thinksound v0.0.16)
- **scripts/demo.sh** - Single video processing script
- **scripts/eval_batch.sh** - Batch processing script

## Current State
- [x] Core inference pipeline (Any2Audio generation)
- [x] Web interface (Gradio demo)
- [x] Batch processing support
- [x] Memory optimization (half-precision support)
- [x] Multi-modal conditioning (Video + Text + Audio)
- [x] Chain-of-Thought reasoning integration
- [ ] Training scripts (planned release)
- [ ] AudioCoT dataset open-sourcing
- [ ] API documentation
- [ ] Multi-scale model variants

## Development Workflow

### Interactive Demo
1. **Start Gradio Interface**: `python app.py`
2. **Upload Video**: Any format (auto-converted to MP4)
3. **Add CoT Description**: Chain-of-thought reasoning prompt
4. **Generate Audio**: Click process button
5. **Download Results**: Audio + synchronized video

### Command Line Processing
1. **Extract Features**: `python extract_latents.py --duration_sec 9 --root videos/ --tsv_path metadata.csv --save-dir results/`
2. **Generate Audio**: `python predict.py --duration-sec 9 --results-dir results/`
3. **Optimize Memory**: Add `--use_half` flag for 50% memory reduction
4. **Batch Process**: Use `scripts/eval_batch.sh` for multiple videos
5. **Validate Output**: Check audio quality and synchronization

## Task Templates

### 1. Model Optimization
1. **Check Memory Usage**: Monitor GPU memory in `predict.py:45-60`
2. **Enable Half-Precision**: Add `--use_half` flag to `extract_latents.py:21-23`
3. **Profile Performance**: Test with sample videos using `scripts/demo.sh`
4. **Validate Quality**: Compare audio output quality before/after optimization

### 2. Feature Extraction Pipeline
1. **Update Video Processing**: Modify `data_utils/v2a_utils/feature_utils_224.py:68-85`
2. **Configure Models**: Update CLIP/T5 settings in `feature_utils_224.py:68-81`
3. **Test Extraction**: Run `extract_latents.py` with sample video
4. **Verify Features**: Check feature dimensions and temporal alignment

### 3. Web Interface Enhancement
1. **Modify Gradio Components**: Update `app.py:80-150` interface elements
2. **Update Processing Logic**: Change `app.py:12-45` run_infer function
3. **Test Locally**: Run `python app.py` and verify functionality
4. **Deploy Changes**: Update Docker container if needed

### 4. Model Configuration
1. **Update Model Config**: Modify `ThinkSound/models/factory.py:50-100`
2. **Adjust Conditioning**: Change `ThinkSound/models/conditioners.py:400-600`
3. **Test Inference**: Run `predict.py` with updated configuration
4. **Validate Audio Quality**: Compare output with baseline model

### 5. Batch Processing Setup
1. **Prepare Dataset**: Create CSV metadata file with video paths and descriptions
2. **Configure Batch Script**: Update `scripts/eval_batch.sh` parameters
3. **Run Extraction**: Execute feature extraction phase for all videos
4. **Generate Audio**: Run prediction phase and collect results
5. **Quality Assessment**: Review generated audio files for consistency

## Anti-Patterns (Avoid These)
❌ **Don't modify model architecture without understanding flow matching** - ThinkSound uses Rectified Flow, not standard DDPM
❌ **Don't skip memory optimization flags on limited GPU** - Use `--use_half` for <16GB VRAM
❌ **Don't process videos without audio format validation** - Use `convert_to_mp4()` in app.py:47-60
❌ **Don't ignore feature extraction dependencies** - CLIP, T5, and SynchFormer must be properly loaded
❌ **Don't modify sampling steps without testing** - 24 steps is optimized for quality/speed balance
❌ **Don't change conditioning dimensions** - Video (1024), Text (1024), T5 (2048) dims are model-dependent
❌ **Don't skip CoT reasoning prompts** - Chain-of-thought descriptions significantly improve generation quality

## Journal Update Requirements
**IMPORTANT**: Update JOURNAL.md regularly throughout our work sessions:
- After completing any significant feature or fix
- When encountering and resolving errors
- At the end of each work session
- When making architectural decisions
- Format: What/Why/How/Issues/Result structure

## Task Management Integration
**How TASKS.md and JOURNAL.md work together**:
1. **Active Work**: TASKS.md tracks current/incomplete tasks with full context
2. **Completed Work**: When tasks complete, they generate JOURNAL.md entries with `|TASK:ID|` tags
3. **History**: JOURNAL.md preserves complete task history even if Claude Code is reinstalled
4. **Context Recovery**: Search JOURNAL.md for `|TASK:` to see all completed tasks over time
5. **Clean Handoffs**: TASKS.md always shows what needs to be resumed or completed

## Version History
- **v0.0.16** - Current release (2025-07-20)
  - Model lightweighted and memory optimized
  - High-throughput audio generation support
  - Half-precision inference capability
- **v0.0.15** - Major release (2025-07-XX)
  - Online demos on HuggingFace Spaces and ModelScope
  - Inference scripts and web interface released
- **v0.0.10** - Initial release (2025-06-XX)
  - ThinkSound paper published on arXiv
  - Core Any2Audio generation framework
  - Chain-of-Thought reasoning integration